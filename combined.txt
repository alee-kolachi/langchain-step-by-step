from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

llm = ChatGroq(model="llama-3.1-8b-instant")
result = llm.invoke("Hey who is the prime minister of paksitan")
print(result.content)"""Project 10 — Agent with Short-Term Memory (Thread-Based)"""

from langchain_groq import ChatGroq
from langchain.tools import tool
from langchain.agents import create_agent
from langchain.agents.middleware import before_model
from langchain.messages import RemoveMessage
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig
from dotenv import load_dotenv

load_dotenv()

#Step 1 : Tools
@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers and return the sum"""
    return a + b

@tool
def multiply_two_numbers(a: int, b: int) -> int:
    """Multiply two numbers and return result"""
    return a * b

#Step 2: Initialize LLM
llm = ChatGroq(model="openai/gpt-oss-120b")

#Step 3: Middleware to trim messages


#Step 4: Create agent with memory
agent = create_agent(
    model=llm,
    tools=[add_numbers, multiply_two_numbers],
    checkpointer=InMemorySaver(),
)

#Step 5: Thread config for short-term memory

# Step 6: Multi-turn conversation
conversation = [
    "Hi, my name is Alice.",
    "Please add 7 + 5.",
    "Now multiply that result by 3.",
    "What was my name again?",
]

for user_input in conversation:
    result = agent.invoke(
        {"messages": [{"role": "user", "content": user_input}]},
        {"configurable": {"thread_id": "1"}}
    )
    print("User:", user_input)
    print("Agent:", result["messages"][-1].content)
    print("---")"""Project 11 — Conversation Summary Memory

Goal: Automatically summarize old messages when context gets too long,
      keeping only the summary + recent messages.
"""

from langchain_groq import ChatGroq
from langchain.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, RemoveMessage, SystemMessage, AIMessage
from typing import Literal
from dotenv import load_dotenv

load_dotenv()

# Step 1: Define Tools
@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers and return the sum"""
    return a + b

@tool
def multiply_numbers(a: int, b: int) -> int:
    """Multiply two numbers and return result"""
    return a * b

# Step 2: Initialize LLM with tools
llm = ChatGroq(model="llama-3.1-8b-instant")
tools = [add_numbers, multiply_numbers]
llm_with_tools = llm.bind_tools(tools)

# Step 3: Define State with Summary
class State(MessagesState):
    summary: str  # Store conversation summary

# Step 4: Define the chatbot node
def call_model(state: State):
    """Main chatbot node that processes user input"""
    summary = state.get("summary", "")
    
    # If we have a summary, add it as context
    if summary:
        system_msg = SystemMessage(
            content=f"Summary of conversation so far: {summary}"
        )
        messages = [system_msg] + state["messages"]
    else:
        messages = state["messages"]
    
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

# Step 5: Route after model
def route_after_model(state: State) -> Literal["tools", "summarize", END]:
    """Route to tools if called, check summarization if not"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # If tools were called, route to tools
    if last_message.tool_calls:
        return "tools"
    
    # Otherwise, check if we need to summarize (threshold: 5 messages)
    # This ensures we summarize BEFORE important info gets deleted
    if len(messages) > 5:
        return "summarize"
    
    return END

# Step 7: Summarization node
def summarize_conversation(state: State):
    """Summarize the conversation and remove old messages"""
    summary = state.get("summary", "")
    
    # Create prompt based on whether summary exists
    if summary:
        summary_prompt = (
            f"Previous summary: {summary}\n\n"
            "Extend the summary by including new information from the messages above. "
            "Preserve ALL important details from the previous summary AND add any new information "
            "like names, preferences, calculations, or facts. Do not lose any critical information:"
        )
    else:
        summary_prompt = (
            "Create a concise summary of the conversation above. "
            "Include ALL important details like:\n"
            "- Person's name if mentioned\n"
            "- Their preferences or interests\n"
            "- Any calculations or results\n"
            "- Key facts discussed"
        )
    
    # Get summary from LLM
    messages = state["messages"] + [HumanMessage(content=summary_prompt)]
    response = llm.invoke(messages)
    
    # Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    
    return {"summary": response.content, "messages": delete_messages}

# Step 8: Build the graph
builder = StateGraph(State)

# Add nodes
builder.add_node("chatbot", call_model)
builder.add_node("tools", ToolNode(tools))
builder.add_node("summarize", summarize_conversation)

# Add edges
builder.add_edge(START, "chatbot")

# Route from chatbot based on tool calls and message count
builder.add_conditional_edges(
    "chatbot",
    route_after_model,
    {"tools": "tools", "summarize": "summarize", END: END}
)

# After tools execute, go back to chatbot
builder.add_edge("tools", "chatbot")

# After summarize, end
builder.add_edge("summarize", END)

# Compile with memory
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

# Step 9: Multi-turn conversation with summarization
conversation = [
    "Hi, my name is Alice.",
    "Please add 7 + 5.",
    "Now multiply that result by 3.",
    "I also love coding in Python.",
    "What programming languages do you know?",
    "Can you add 10 + 20 for me?",
    "What was my name again?",  # This should use the summary
]

config = {"configurable": {"thread_id": "1"}}

print("=" * 60)
print("CONVERSATION WITH SUMMARY MEMORY")
print("=" * 60)

for user_input in conversation:
    result = graph.invoke(
        {"messages": [HumanMessage(content=user_input)]},
        config
    )
    
    # Get the last AI message
    ai_messages = [m for m in result['messages'] if isinstance(m, AIMessage)]
    last_ai_msg = ai_messages[-1] if ai_messages else None
    
    print(f"\nUser: {user_input}")
    if last_ai_msg:
        print(f"Agent: {last_ai_msg.content}")
    
    # Show summary if it was just created
    if result.get("summary"):
        print(f"[Summary created/updated]")
    print("-" * 60)

# Step 10: Check final state
final_state = graph.get_state(config)
print("\n" + "=" * 60)
print("FINAL STATE")
print("=" * 60)
print(f"\nMessages in memory: {len(final_state.values['messages'])}")
print(f"\nFull Summary:\n{final_state.values.get('summary', 'No summary yet')}")from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

llm = ChatGroq(model="llama-3.1-8b-instant")
response = llm.invoke("Give a 1-line tagline for 'Smartwatch'.")
tagline = response.content.strip()
print(tagline)"""Project 4 — Two-Step Chain

Goal: Run one output into another prompt."""

from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

llm = ChatGroq(model="llama-3.1-8b-instant")
tagline = llm.invoke("Give 1 line tagline for my new product i.e Hair Oil")

explanation = llm.invoke(f"Explain why this tagline is effective: {tagline}").content.strip()

print("Tagline:", tagline)
print("Explanation:", explanation)"""Project 5 — Mini Knowledge Base

Goal: Store a few text snippets and retrieve manually."""

docs = [
    "LangChain helps build LLM applications.",
    "RAG improves LLM responses with external info.",
    "FAISS is a vector search library."
]

query = "What is LangChain?"

for doc in docs:
    if "LangChain" in doc:
        context = doc
        break

from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

llm = ChatGroq(model="llama-3.1-8b-instant")
response = llm.invoke(f"Based on this info: {context}, Explain langchain simply.")

print(response.content.strip())"""Project 6 — Vector Store Retrieval

Goal: Use embeddings + FAISS for smarter retrieval."""

from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

docs = [
    "LangChain helps build LLM applications.",
    "RAG improves LLM responses with external info.",
    "FAISS is a vector search library."
]

#Create Embeddings
model_name = "all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=model_name)
print(f"Embeddings: {embeddings}")
vectorstore = FAISS.from_texts(docs, embeddings)
print(f"VectorStore: {vectorstore}")

#Retreieve
retriever = vectorstore.as_retriever()
results = retriever.invoke("What is RAG")
context = results[0].page_content
print(f"Context: {context}")


#Ask LLM
llm = ChatGroq(model="llama-3.1-8b-instant")
response = llm.invoke(f"Explain in short using this {context}").content.strip()

print(response)

"""Project 7 — Simple RAG Chain

Goal: Combine retrieval + LLM into one flow."""

from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

data = [
    "We're a team of 50 people, all working remotely",
    "We allow only 5 holidays per year",
    "You get bonus every 6 months"
]

#Create embeddings
model_name = "all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=model_name)
vectorstore = FAISS.from_texts(data, embeddings)
retriever = vectorstore.as_retriever()

#user query
query = "How many leaves do you allow per year?"
results = retriever.invoke(query)
context = results[0].page_content

#llm answer
llm = ChatGroq(model="llama-3.1-8b-instant")
response = llm.invoke(f"Using this context: {context}, answer the user query: {query}")
print(response.content.strip())"""Project 8 — Basic Tool Agent

Goal: Let LLM call a Python function."""

from langchain_groq import ChatGroq
from langchain.tools import tool
from langchain.agents import create_agent
from langchain.messages import AIMessage
from dotenv import load_dotenv

load_dotenv()

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two integers together."""
    return a+b

llm = ChatGroq(model="llama-3.1-8b-instant")
agent = create_agent(
    model=llm,
    tools=[add_numbers],
    system_prompt="You can use a tool to add two numbers."
)

result = agent.invoke(
    {
        "messages": [
            {
                "role": "user",
                "content": "What is 2+45"
            }
        ]
    }
)

for msg in result["messages"]:
    if isinstance(msg, AIMessage):
        print(msg.content)

        """Project 9 — Multi-Tool Agent (Add + Multiply + Retrieve)"""

from langchain_groq import ChatGroq
from langchain.tools import tool
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.messages import AIMessage
from langchain.agents import create_agent
from typing import Any
from dotenv import load_dotenv

load_dotenv()

#Step1. Defining tools

@tool
def add_numbers(a: int, b: int) -> int:
    """This function adds two numbers and returns the total result."""
    return a+b

@tool
def multiply_numbers(a: int, b: int) -> int:
    """This function multiply two numbers and return the total result."""
    return a*b

docs = [
    "We're a team of 50 people, all working remotely",
    "We allow only 5 holidays per year",
    "You get bonus every 6 months"
]

model_name = "all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=model_name)
vectorstore = FAISS.from_texts(docs, embeddings)

@tool
def retrieve_info(query: str) -> str:
    """Retreieves similar information from docs as per given query"""
    results = vectorstore.similarity_search(query, k=2)
    return "\n".join([r.page_content for r in results])

#Initialize LLM
llm = ChatGroq(model="openai/gpt-oss-120b")

#Create tool agent
agent = create_agent(
    model=llm,
    tools=[add_numbers, multiply_numbers, retrieve_info],
    system_prompt="""You can add, multiply, or retrieve information from the knowledge base to answer the user
                    You MUST use tools for math. You MUST NOT answer math yourself.
                    You MUST retrieve information from vectorstore when the user asks about the company."""
)

#Ask the agent
questions = [
    "How much is 2+4",
    "What is 4*4",
    "A friend was asking me question that got me confused, he asked me how much are the total eggs in 5 days if one hen lays 5 eggs daily",
    "Explain our holiday policy at my company."
]

for question in questions:
    result = agent.invoke({
        "messages": [{
            "role": "user",
            "content": question
        }]
    })
    #Clean AI output
    final_message = None
    for msg in result["messages"]:
        if isinstance(msg, AIMessage):
            final_message = msg  # overwrite until last one

    print(f"Question: {question}\nAnswer: {final_message.content}")
# project12_simple_chat.py

from langchain_groq import ChatGroq
from langchain.agents import create_agent
from langchain.messages import AIMessage
from dotenv import load_dotenv

load_dotenv()

# Step 1: Initialize LLM
llm = ChatGroq(model="openai/gpt-oss-120b")

# Step 2: Create a simple agent without tools or memory
agent = create_agent(
    model=llm,
    system_prompt="You are a helpful assistant. Answer questions naturally."
)

# Step 3: Sample conversation
questions = [
    "Hello, how are you?",
    "Can you explain what LangChain is?",
    "Give me a joke."
]

for q in questions:
    result = agent.invoke({"messages": [{"role": "user", "content": q}]})
    # Extract only the final AI answer
    final_answer = result["messages"][-1].content
    print(f"Q: {q}\nA: {final_answer}\n")
